{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Zen of Python, by Tim Peters\n",
      "\n",
      "Beautiful is better than ugly.\n",
      "Explicit is better than implicit.\n",
      "Simple is better than complex.\n",
      "Complex is better than complicated.\n",
      "Flat is better than nested.\n",
      "Sparse is better than dense.\n",
      "Readability counts.\n",
      "Special cases aren't special enough to break the rules.\n",
      "Although practicality beats purity.\n",
      "Errors should never pass silently.\n",
      "Unless explicitly silenced.\n",
      "In the face of ambiguity, refuse the temptation to guess.\n",
      "There should be one-- and preferably only one --obvious way to do it.\n",
      "Although that way may not be obvious at first unless you're Dutch.\n",
      "Now is better than never.\n",
      "Although never is often better than *right* now.\n",
      "If the implementation is hard to explain, it's a bad idea.\n",
      "If the implementation is easy to explain, it may be a good idea.\n",
      "Namespaces are one honking great idea -- let's do more of those!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/margaretbrewster/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/margaretbrewster/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/margaretbrewster/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/margaretbrewster/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from this import s\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from feature_engineering import refuting_features, polarity_features, hand_features, gen_or_load_feats, tfidf_features\n",
    "from feature_engineering import word_overlap_features\n",
    "from utils.dataset import DataSet\n",
    "from utils.generate_test_splits import kfold_split, get_stances_for_folds\n",
    "from utils.score import report_score, LABELS, score_submission\n",
    "\n",
    "from utils.system import parse_params, check_version\n",
    "from csv import DictReader\n",
    "import pandas\n",
    "\n",
    "import gensim\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from tqdm import tqdm\n",
    "from nltk import tokenize\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM,Dense,Dropout,Embedding,CuDNNLSTM,Bidirectional, Flatten\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "hyperparam = {\n",
    "    'batch_size': 200,\n",
    "    'max_vocab_size': 20000,\n",
    "    'embedding_dim': 100,\n",
    "    'dropout_rate': 0.3,\n",
    "    'learning_rate': 0.1,\n",
    "    'n_epochs': 10,\n",
    "    'max_length': 100\n",
    "}\n",
    "\n",
    "## Returns the feature array used to train the sequential model\n",
    "def generate_features(stances,dataset,name):\n",
    "    h, b, y = [],[],[]\n",
    "\n",
    "    for stance in stances:\n",
    "        y.append(LABELS.index(stance['Stance']))\n",
    "        h.append(stance['Headline'])\n",
    "        b.append(dataset.articles[stance['Body ID']])\n",
    "\n",
    "    X_overlap = gen_or_load_feats(word_overlap_features, h, b, \"features/overlap.\"+name+\".npy\")\n",
    "    X_refuting = gen_or_load_feats(refuting_features, h, b, \"features/refuting.\"+name+\".npy\")\n",
    "    X_polarity = gen_or_load_feats(polarity_features, h, b, \"features/polarity.\"+name+\".npy\")\n",
    "    X_hand = gen_or_load_feats(hand_features, h, b, \"features/hand.\"+name+\".npy\")\n",
    "    X_tfidf = gen_or_load_feats(tfidf_features, h, b, \"features/tfidf.\"+name+\".npy\")\n",
    "    X = np.c_[X_hand, X_polarity, X_refuting, X_overlap, X_tfidf]\n",
    "\n",
    "    return X,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset\n",
      "Total stances: 49972\n",
      "Total bodies: 1683\n",
      "Reading dataset\n",
      "Total stances: 25413\n",
      "Total bodies: 904\n"
     ]
    }
   ],
   "source": [
    "# check_version()\n",
    "# parse_params()\n",
    "\n",
    "#Load the training dataset and generate folds\n",
    "d = DataSet()\n",
    "folds,hold_out = kfold_split(d,n_folds=10)\n",
    "fold_stances, hold_out_stances = get_stances_for_folds(d,folds,hold_out)\n",
    "\n",
    "# Load the competition dataset\n",
    "competition_dataset = DataSet(\"competition_test\")\n",
    "X_competition, y_competition = generate_features(competition_dataset.stances, competition_dataset, \"competition\")\n",
    "\n",
    "h, b = [], []\n",
    "for stance in competition_dataset.stances:\n",
    "    h.append(stance['Headline'])\n",
    "    b.append(stance['Body ID'])\n",
    "\n",
    "answers = {'Headline': h, 'Body ID': b, 'Stance': []}\n",
    "\n",
    "Xs = dict()\n",
    "ys = dict()\n",
    "\n",
    "# Load/Precompute all features now\n",
    "X_holdout,y_holdout = generate_features(hold_out_stances,d,\"holdout\")\n",
    "for fold in fold_stances:\n",
    "    Xs[fold],ys[fold] = generate_features(fold_stances[fold],d,str(fold))\n",
    "\n",
    "\n",
    "best_score = 0\n",
    "best_fold = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = d.articles.values()\n",
    "sentences = []\n",
    "for article in articles:\n",
    "    sentences += tokenize.sent_tokenize(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build tokenizer\n",
    "word_seq = [text_to_word_sequence(sent) for sent in sentences]\n",
    "token = Tokenizer(num_words=hyperparam['max_vocab_size'])\n",
    "token.fit_on_texts([' '.join(seq[:hyperparam['max_length']]) for seq in word_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400001it [00:10, 38954.78it/s]\n"
     ]
    }
   ],
   "source": [
    "#build glove embedding vector\n",
    "embedding_vector = {}\n",
    "f = open('./glove/glove.6B.100d.txt')\n",
    "for line in tqdm(f):\n",
    "    value = line.split(' ')\n",
    "    word = value[0]\n",
    "    coef = np.array(value[1:],dtype = 'float32')\n",
    "    embedding_vector[word] = coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 27368/27368 [00:00<00:00, 307702.67it/s]\n"
     ]
    }
   ],
   "source": [
    "## create embedding matrix from gloVe word vectors\n",
    "## gloVe is avaliable for download at https://nlp.stanford.edu/projects/glove/\n",
    "vocab_size = len(token.word_index.items()) + 1\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size,100))\n",
    "for word,i in tqdm(token.word_index.items()):\n",
    "    embedding_value = embedding_vector.get(word)\n",
    "    if embedding_value is not None:\n",
    "        embedding_matrix[i] = embedding_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(\n",
    "    input_dim=len(embedding_matrix),\n",
    "    output_dim=hyperparam['embedding_dim'],\n",
    "    weights=[embedding_matrix],\n",
    "    trainable = True\n",
    "))\n",
    "model.add(Bidirectional(LSTM(100, return_sequences=False, name='Bidrectional_lstm_layer1')))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32,activation = 'relu'))\n",
    "# model.add(Dense(1,activation = 'sigmoid'))\n",
    "# model.add(Dense(1,activation = 'relu'))\n",
    "\n",
    "##we tested drop out rates  from 0 to 0.6\n",
    "model.add(Dropout(rate=0.1, name='dropout_1')) \n",
    "model.add(Dense(4,activation='softmax'))\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 3 ... 3 3 3]\n",
      "Epoch 1/10\n",
      "182/182 [==============================] - 64s 332ms/step - loss: 0.4780 - accuracy: 0.8380 - val_loss: 0.4197 - val_accuracy: 0.8282\n",
      "Epoch 2/10\n",
      "182/182 [==============================] - 58s 319ms/step - loss: 0.3799 - accuracy: 0.8651 - val_loss: 0.3919 - val_accuracy: 0.8370\n",
      "Epoch 3/10\n",
      "182/182 [==============================] - 59s 326ms/step - loss: 0.3681 - accuracy: 0.8687 - val_loss: 0.3856 - val_accuracy: 0.8443\n",
      "Epoch 4/10\n",
      "182/182 [==============================] - 61s 338ms/step - loss: 0.3583 - accuracy: 0.8728 - val_loss: 0.3957 - val_accuracy: 0.8384\n",
      "Epoch 5/10\n",
      "182/182 [==============================] - 58s 316ms/step - loss: 0.3540 - accuracy: 0.8749 - val_loss: 0.3871 - val_accuracy: 0.8413\n",
      "Epoch 6/10\n",
      "182/182 [==============================] - 60s 331ms/step - loss: 0.3493 - accuracy: 0.8761 - val_loss: 0.4016 - val_accuracy: 0.8355\n",
      "Epoch 7/10\n",
      "182/182 [==============================] - 58s 318ms/step - loss: 0.3472 - accuracy: 0.8764 - val_loss: 0.4076 - val_accuracy: 0.8384\n",
      "Epoch 8/10\n",
      "182/182 [==============================] - 57s 315ms/step - loss: 0.3464 - accuracy: 0.8768 - val_loss: 0.3816 - val_accuracy: 0.8384\n",
      "Epoch 9/10\n",
      "182/182 [==============================] - 60s 327ms/step - loss: 0.3436 - accuracy: 0.8776 - val_loss: 0.3766 - val_accuracy: 0.8370\n",
      "Epoch 10/10\n",
      " 55/182 [========>.....................] - ETA: 39s - loss: 0.3396 - accuracy: 0.8795"
     ]
    }
   ],
   "source": [
    "# Classifier for each fold\n",
    "for fold in fold_stances:\n",
    "    ids = list(range(len(folds)))\n",
    "    del ids[fold]\n",
    "\n",
    "    X_train = np.vstack(tuple([Xs[i] for i in ids]))\n",
    "    y_train = np.hstack(tuple([ys[i] for i in ids]))\n",
    "    \n",
    "    print(y_train)\n",
    "\n",
    "    X_test = Xs[fold]\n",
    "    y_test = ys[fold]\n",
    "    \n",
    "    ## divide into a larger train set and smaller test set \n",
    "    X_val = np.array(X_test[:(len(X_test) // 6)])\n",
    "    y_val = np.array(y_test[:(len(X_test) // 6)])\n",
    "    x_test = np.array(X_test[(len(X_test) // 6):])\n",
    "    y_test = np.array(y_test[(len(X_test) // 6):])\n",
    "    \n",
    "    \n",
    "    \n",
    "#  new model\n",
    "    model.fit(X_train, y_train,\n",
    "              batch_size=hyperparam['batch_size'],\n",
    "              epochs=10,\n",
    "              validation_data=(X_val, y_val),\n",
    "              verbose=1)\n",
    "  \n",
    "    predicted = [LABELS[np.argmax(a, axis = 0)] for a in model.predict(x_test)]\n",
    "    actual = [LABELS[int(a)] for a in y_test]\n",
    "    \n",
    "    report_score(actual,predicted)\n",
    "\n",
    "    fold_score, _ = score_submission(actual, predicted)\n",
    "    max_fold_score, _ = score_submission(actual, actual)\n",
    "\n",
    "    score = fold_score/max_fold_score\n",
    "\n",
    "    print(\"Score for fold \"+ str(fold) + \" was - \" + str(score))\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_fold = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run on Holdout set and report the final score on the holdout set\n",
    "predicted = [LABELS[np.argmax(a, axis = -1)] for a in best_fold.predict(X_holdout)]\n",
    "actual = [LABELS[int(a)] for a in y_holdout]\n",
    "\n",
    "print(best_fold.predict(X_holdout)) \n",
    "print(predicted) \n",
    "\n",
    "print(y_competition) \n",
    "\n",
    "print(\"Scores on the dev set\")\n",
    "report_score(actual,predicted)\n",
    "\n",
    "#Run on competition dataset\n",
    "predicted = [LABELS[np.argmax(a, axis = 0)] for a in best_fold.predict(X_competition)]\n",
    "print(X_competition) \n",
    "\n",
    "answers[\"Stance\"] = predicted\n",
    "answers = pandas.DataFrame(answers)\n",
    "answers.to_csv('answer.csv', index=False, encoding='utf-8')\n",
    "actual = [LABELS[int(a)] for a in y_competition]\n",
    "print(\"Scores on the test set\")\n",
    "report_score(actual,predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
